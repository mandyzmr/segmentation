{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe7837d-6ee0-44ae-b751-42015f6f54c4",
   "metadata": {},
   "source": [
    ">### DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3204ec8-0e1a-4565-9ae6-a38fd8f6905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# Plotting\n",
    "import imageio\n",
    "import PIL\n",
    "from PIL import ImageDraw, Image\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.sans-serif']=['Arial Unicode MS'] \n",
    "plt.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "# Framework\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada467d9-b611-4c71-9483-4d84463172b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pytorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648fa8b-292c-4295-b005-8bbdc4b8e5d7",
   "metadata": {},
   "source": [
    "沿用[ResNet Blocks](#resnet_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9beb693-d0e7-434d-9bc7-e05e9b14fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_Atrous(nn.Module): #没有FC，32倍下采样，用作backbone\n",
    "    def __init__(self, name, in_chans=3, block=None, n_blocks=None,\n",
    "                 dilation_rate=[1,1,1], #若增加空洞卷积，layer 5直接指定每个block的d\n",
    "                 os=16): #output_stride 可选下采样倍数os=8/16，传统resnet都是2**5=32倍downsampling \n",
    "        super(ResNet_Atrous, self).__init__() \n",
    "       \n",
    "        # 结构：使用自定义的结构，或者经典结构\n",
    "        self.structure = name\n",
    "        self.block = block #用basicblock还是bottleneck\n",
    "        self.n_blocks = n_blocks #每个block循环次数\n",
    "        self.get_structure() #重新整理结构\n",
    "        self.strides = self.get_strides(os, dilation_rate)\n",
    "            \n",
    "        # Stem\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                nn.BatchNorm2d(64), \n",
    "                nn.ReLU(inplace=True))\n",
    "        self.pool = nn.MaxPool2d(3, 2, padding=1) #4倍downsample\n",
    "        \n",
    "        #除了layer2因为有事先pooling降维之外，其余layer都是第一个block的stride=2降维，其余blocks的stride=1\n",
    "        self.in_chans = 64 #后续不断更新\n",
    "        self.layer2 = self.build_blocks(64, self.n_blocks[0], stride=self.strides[0])         \n",
    "        self.layer3 = self.build_blocks(128, self.n_blocks[1], stride=self.strides[1]) #8倍downsample，根据下面layers决定最终downsample倍数 \n",
    "        self.layer4 = self.build_blocks(256, self.n_blocks[2], stride=self.strides[2],\n",
    "                                        dilation_rate=16//os) #可以对每个block做空洞d，因为不同resnet结构layer4的n_blocks不同，在函数另外构造d列表 \n",
    "        self.layer5 = self.build_blocks(512, self.n_blocks[3], stride=self.strides[3],\n",
    "                                        dilation_rate=[i*16//os for i in dilation_rate]) #若下采样os=8倍为止，那么这层的dilated_rate要乘以2\n",
    "        \n",
    "        #若用空洞卷积，还可以自定义额外增加几次最后一个layer结构\n",
    "        self.layer6 = self.build_blocks(512, self.n_blocks[3], stride=self.strides[3],\n",
    "                                        dilation_rate=[i*16//os for i in dilation_rate]) #同layer5\n",
    "        self.layer7 = self.build_blocks(512, self.n_blocks[3], stride=self.strides[3],\n",
    "                                        dilation_rate=[i*16//os for i in dilation_rate]) \n",
    "     \n",
    "    \n",
    "    def get_structure(self):\n",
    "        versions = {'resnet18':(BasicBlock,[2,2,2,2]), \n",
    "                     'resnet34':(BasicBlock,[3,4,6,3]),\n",
    "                     'resnet50':(BottleNeck,[3,4,6,3]),\n",
    "                     'resnet101':(BottleNeck,[3,4,23,3]),\n",
    "                     'resnet152':(BottleNeck,[3,8,36,3])}\n",
    "        \n",
    "        if not self.block or not self.n_blocks: #如果任意一项没有定义\n",
    "            self.block, self.n_blocks = versions[self.structure]\n",
    "        \n",
    "        \n",
    "    def get_strides(self, os, dilation_rate):\n",
    "        if os==16 and dilation_rate==[1,1,1]: #代表用传统的32倍downsample，不用空洞卷积\n",
    "            strides=[1,2,2,2]\n",
    "        elif os==16: #若16倍downsample，仅最后layer 5 strides=1，可以用空洞卷积\n",
    "            strides=[1,2,2,1] \n",
    "        elif os==8: #若8倍downsample，最后layer 4-5 strides=1，可以用空洞卷积\n",
    "            strides=[1,2,1,1] \n",
    "        return strides\n",
    "    \n",
    "    \n",
    "    def build_blocks(self, out_chans, n_blocks, stride, dilation_rate=1):      \n",
    "        if isinstance(dilation_rate, int): #根据具体blocks数构造\n",
    "            dilation_rate=[dilation_rate]*n_blocks\n",
    "         \n",
    "        blocks = [self.block(self.in_chans, out_chans, stride=stride, dilation_rate=dilation_rate[0])]\n",
    "        self.in_chans = out_chans*self.block.expansion\n",
    "        for i in range(1, n_blocks):   #跳过第一个block          \n",
    "            blocks.append(self.block(self.in_chans, out_chans, stride=1, dilation_rate=dilation_rate[i])) \n",
    "        blocks = nn.Sequential(*blocks) \n",
    "        return blocks\n",
    "\n",
    "            \n",
    "    def forward(self, inputs): \n",
    "        f1 = self.layer1(inputs) #2\n",
    "        p = self.pool(f1)\n",
    "        f2 = self.layer2(p) #4\n",
    "        f3 = self.layer3(f2) #8        \n",
    "        f4 = self.layer4(f3) #8/16        \n",
    "        f5 = self.layer5(f4) #8/16/32\n",
    "        f5 = self.layer6(f5)\n",
    "        f5 = self.layer7(f5)\n",
    "        return f1, f2, f3, f4, f5 #返回4个bridges和一个bottleneck\n",
    "    \n",
    "    \n",
    "    def load_pretrained(self, model_path): #但是名字对不上咋办\n",
    "        old_dict = model_zoo.load_url(model_path)\n",
    "        model_dict = model.state_dict()\n",
    "        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n",
    "        model_dict.update(old_dict) #合并字典\n",
    "        model.load_state_dict(model_dict) #把pretrained的weights对应填充到自定义模型\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade3c7fb-5e60-4d10-a198-2e3b7072c02c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 192, 512]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 192, 512]             128\n",
      "              ReLU-3         [-1, 64, 192, 512]               0\n",
      "         MaxPool2d-4          [-1, 64, 96, 256]               0\n",
      "            Conv2d-5          [-1, 64, 96, 256]           4,096\n",
      "       BatchNorm2d-6          [-1, 64, 96, 256]             128\n",
      "              ReLU-7          [-1, 64, 96, 256]               0\n",
      "            Conv2d-8          [-1, 64, 96, 256]          36,864\n",
      "       BatchNorm2d-9          [-1, 64, 96, 256]             128\n",
      "             ReLU-10          [-1, 64, 96, 256]               0\n",
      "           Conv2d-11         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-12         [-1, 256, 96, 256]             512\n",
      "           Conv2d-13         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-14         [-1, 256, 96, 256]             512\n",
      "       BottleNeck-15         [-1, 256, 96, 256]               0\n",
      "           Conv2d-16          [-1, 64, 96, 256]          16,384\n",
      "      BatchNorm2d-17          [-1, 64, 96, 256]             128\n",
      "             ReLU-18          [-1, 64, 96, 256]               0\n",
      "           Conv2d-19          [-1, 64, 96, 256]          36,864\n",
      "      BatchNorm2d-20          [-1, 64, 96, 256]             128\n",
      "             ReLU-21          [-1, 64, 96, 256]               0\n",
      "           Conv2d-22         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-23         [-1, 256, 96, 256]             512\n",
      "       BottleNeck-24         [-1, 256, 96, 256]               0\n",
      "           Conv2d-25          [-1, 64, 96, 256]          16,384\n",
      "      BatchNorm2d-26          [-1, 64, 96, 256]             128\n",
      "             ReLU-27          [-1, 64, 96, 256]               0\n",
      "           Conv2d-28          [-1, 64, 96, 256]          36,864\n",
      "      BatchNorm2d-29          [-1, 64, 96, 256]             128\n",
      "             ReLU-30          [-1, 64, 96, 256]               0\n",
      "           Conv2d-31         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-32         [-1, 256, 96, 256]             512\n",
      "       BottleNeck-33         [-1, 256, 96, 256]               0\n",
      "           Conv2d-34         [-1, 128, 96, 256]          32,768\n",
      "      BatchNorm2d-35         [-1, 128, 96, 256]             256\n",
      "             ReLU-36         [-1, 128, 96, 256]               0\n",
      "           Conv2d-37         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-38         [-1, 128, 48, 128]             256\n",
      "             ReLU-39         [-1, 128, 48, 128]               0\n",
      "           Conv2d-40         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-41         [-1, 512, 48, 128]           1,024\n",
      "           Conv2d-42         [-1, 512, 48, 128]         131,072\n",
      "      BatchNorm2d-43         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-44         [-1, 512, 48, 128]               0\n",
      "           Conv2d-45         [-1, 128, 48, 128]          65,536\n",
      "      BatchNorm2d-46         [-1, 128, 48, 128]             256\n",
      "             ReLU-47         [-1, 128, 48, 128]               0\n",
      "           Conv2d-48         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-49         [-1, 128, 48, 128]             256\n",
      "             ReLU-50         [-1, 128, 48, 128]               0\n",
      "           Conv2d-51         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-52         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-53         [-1, 512, 48, 128]               0\n",
      "           Conv2d-54         [-1, 128, 48, 128]          65,536\n",
      "      BatchNorm2d-55         [-1, 128, 48, 128]             256\n",
      "             ReLU-56         [-1, 128, 48, 128]               0\n",
      "           Conv2d-57         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-58         [-1, 128, 48, 128]             256\n",
      "             ReLU-59         [-1, 128, 48, 128]               0\n",
      "           Conv2d-60         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-61         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-62         [-1, 512, 48, 128]               0\n",
      "           Conv2d-63         [-1, 128, 48, 128]          65,536\n",
      "      BatchNorm2d-64         [-1, 128, 48, 128]             256\n",
      "             ReLU-65         [-1, 128, 48, 128]               0\n",
      "           Conv2d-66         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-67         [-1, 128, 48, 128]             256\n",
      "             ReLU-68         [-1, 128, 48, 128]               0\n",
      "           Conv2d-69         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-70         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-71         [-1, 512, 48, 128]               0\n",
      "           Conv2d-72         [-1, 256, 48, 128]         131,072\n",
      "      BatchNorm2d-73         [-1, 256, 48, 128]             512\n",
      "             ReLU-74         [-1, 256, 48, 128]               0\n",
      "           Conv2d-75          [-1, 256, 24, 64]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 24, 64]             512\n",
      "             ReLU-77          [-1, 256, 24, 64]               0\n",
      "           Conv2d-78         [-1, 1024, 24, 64]         262,144\n",
      "      BatchNorm2d-79         [-1, 1024, 24, 64]           2,048\n",
      "           Conv2d-80         [-1, 1024, 24, 64]         524,288\n",
      "      BatchNorm2d-81         [-1, 1024, 24, 64]           2,048\n",
      "       BottleNeck-82         [-1, 1024, 24, 64]               0\n",
      "           Conv2d-83          [-1, 256, 24, 64]         262,144\n",
      "      BatchNorm2d-84          [-1, 256, 24, 64]             512\n",
      "             ReLU-85          [-1, 256, 24, 64]               0\n",
      "           Conv2d-86          [-1, 256, 24, 64]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 24, 64]             512\n",
      "             ReLU-88          [-1, 256, 24, 64]               0\n",
      "           Conv2d-89         [-1, 1024, 24, 64]         262,144\n",
      "      BatchNorm2d-90         [-1, 1024, 24, 64]           2,048\n",
      "       BottleNeck-91         [-1, 1024, 24, 64]               0\n",
      "           Conv2d-92          [-1, 256, 24, 64]         262,144\n",
      "      BatchNorm2d-93          [-1, 256, 24, 64]             512\n",
      "             ReLU-94          [-1, 256, 24, 64]               0\n",
      "           Conv2d-95          [-1, 256, 24, 64]         589,824\n",
      "      BatchNorm2d-96          [-1, 256, 24, 64]             512\n",
      "             ReLU-97          [-1, 256, 24, 64]               0\n",
      "           Conv2d-98         [-1, 1024, 24, 64]         262,144\n",
      "      BatchNorm2d-99         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-100         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-101          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 24, 64]             512\n",
      "            ReLU-103          [-1, 256, 24, 64]               0\n",
      "          Conv2d-104          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 24, 64]             512\n",
      "            ReLU-106          [-1, 256, 24, 64]               0\n",
      "          Conv2d-107         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-109         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-110          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-111          [-1, 256, 24, 64]             512\n",
      "            ReLU-112          [-1, 256, 24, 64]               0\n",
      "          Conv2d-113          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-114          [-1, 256, 24, 64]             512\n",
      "            ReLU-115          [-1, 256, 24, 64]               0\n",
      "          Conv2d-116         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-117         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-118         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-119          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-120          [-1, 256, 24, 64]             512\n",
      "            ReLU-121          [-1, 256, 24, 64]               0\n",
      "          Conv2d-122          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 24, 64]             512\n",
      "            ReLU-124          [-1, 256, 24, 64]               0\n",
      "          Conv2d-125         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-127         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-128          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-129          [-1, 256, 24, 64]             512\n",
      "            ReLU-130          [-1, 256, 24, 64]               0\n",
      "          Conv2d-131          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-132          [-1, 256, 24, 64]             512\n",
      "            ReLU-133          [-1, 256, 24, 64]               0\n",
      "          Conv2d-134         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-135         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-136         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-137          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-138          [-1, 256, 24, 64]             512\n",
      "            ReLU-139          [-1, 256, 24, 64]               0\n",
      "          Conv2d-140          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-141          [-1, 256, 24, 64]             512\n",
      "            ReLU-142          [-1, 256, 24, 64]               0\n",
      "          Conv2d-143         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-144         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-145         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-146          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-147          [-1, 256, 24, 64]             512\n",
      "            ReLU-148          [-1, 256, 24, 64]               0\n",
      "          Conv2d-149          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-150          [-1, 256, 24, 64]             512\n",
      "            ReLU-151          [-1, 256, 24, 64]               0\n",
      "          Conv2d-152         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-153         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-154         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-155          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-156          [-1, 256, 24, 64]             512\n",
      "            ReLU-157          [-1, 256, 24, 64]               0\n",
      "          Conv2d-158          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-159          [-1, 256, 24, 64]             512\n",
      "            ReLU-160          [-1, 256, 24, 64]               0\n",
      "          Conv2d-161         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-162         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-163         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-164          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-165          [-1, 256, 24, 64]             512\n",
      "            ReLU-166          [-1, 256, 24, 64]               0\n",
      "          Conv2d-167          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-168          [-1, 256, 24, 64]             512\n",
      "            ReLU-169          [-1, 256, 24, 64]               0\n",
      "          Conv2d-170         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-171         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-172         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-173          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-174          [-1, 256, 24, 64]             512\n",
      "            ReLU-175          [-1, 256, 24, 64]               0\n",
      "          Conv2d-176          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-177          [-1, 256, 24, 64]             512\n",
      "            ReLU-178          [-1, 256, 24, 64]               0\n",
      "          Conv2d-179         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-180         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-181         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-182          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-183          [-1, 256, 24, 64]             512\n",
      "            ReLU-184          [-1, 256, 24, 64]               0\n",
      "          Conv2d-185          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-186          [-1, 256, 24, 64]             512\n",
      "            ReLU-187          [-1, 256, 24, 64]               0\n",
      "          Conv2d-188         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-189         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-190         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-191          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 24, 64]             512\n",
      "            ReLU-193          [-1, 256, 24, 64]               0\n",
      "          Conv2d-194          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 24, 64]             512\n",
      "            ReLU-196          [-1, 256, 24, 64]               0\n",
      "          Conv2d-197         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-199         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-200          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-201          [-1, 256, 24, 64]             512\n",
      "            ReLU-202          [-1, 256, 24, 64]               0\n",
      "          Conv2d-203          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-204          [-1, 256, 24, 64]             512\n",
      "            ReLU-205          [-1, 256, 24, 64]               0\n",
      "          Conv2d-206         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-207         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-208         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-209          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-210          [-1, 256, 24, 64]             512\n",
      "            ReLU-211          [-1, 256, 24, 64]               0\n",
      "          Conv2d-212          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-213          [-1, 256, 24, 64]             512\n",
      "            ReLU-214          [-1, 256, 24, 64]               0\n",
      "          Conv2d-215         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-216         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-217         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-218          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-219          [-1, 256, 24, 64]             512\n",
      "            ReLU-220          [-1, 256, 24, 64]               0\n",
      "          Conv2d-221          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-222          [-1, 256, 24, 64]             512\n",
      "            ReLU-223          [-1, 256, 24, 64]               0\n",
      "          Conv2d-224         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-225         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-226         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-227          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-228          [-1, 256, 24, 64]             512\n",
      "            ReLU-229          [-1, 256, 24, 64]               0\n",
      "          Conv2d-230          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-231          [-1, 256, 24, 64]             512\n",
      "            ReLU-232          [-1, 256, 24, 64]               0\n",
      "          Conv2d-233         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-234         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-235         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-236          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-237          [-1, 256, 24, 64]             512\n",
      "            ReLU-238          [-1, 256, 24, 64]               0\n",
      "          Conv2d-239          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-240          [-1, 256, 24, 64]             512\n",
      "            ReLU-241          [-1, 256, 24, 64]               0\n",
      "          Conv2d-242         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-243         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-244         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-245          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-246          [-1, 256, 24, 64]             512\n",
      "            ReLU-247          [-1, 256, 24, 64]               0\n",
      "          Conv2d-248          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-249          [-1, 256, 24, 64]             512\n",
      "            ReLU-250          [-1, 256, 24, 64]               0\n",
      "          Conv2d-251         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-252         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-253         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-254          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-255          [-1, 256, 24, 64]             512\n",
      "            ReLU-256          [-1, 256, 24, 64]               0\n",
      "          Conv2d-257          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-258          [-1, 256, 24, 64]             512\n",
      "            ReLU-259          [-1, 256, 24, 64]               0\n",
      "          Conv2d-260         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-261         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-262         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-263          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-264          [-1, 256, 24, 64]             512\n",
      "            ReLU-265          [-1, 256, 24, 64]               0\n",
      "          Conv2d-266          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-267          [-1, 256, 24, 64]             512\n",
      "            ReLU-268          [-1, 256, 24, 64]               0\n",
      "          Conv2d-269         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-270         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-271         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-272          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-273          [-1, 256, 24, 64]             512\n",
      "            ReLU-274          [-1, 256, 24, 64]               0\n",
      "          Conv2d-275          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-276          [-1, 256, 24, 64]             512\n",
      "            ReLU-277          [-1, 256, 24, 64]               0\n",
      "          Conv2d-278         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-279         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-280         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-281          [-1, 512, 24, 64]         524,288\n",
      "     BatchNorm2d-282          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-283          [-1, 512, 24, 64]               0\n",
      "          Conv2d-284          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-285          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-286          [-1, 512, 24, 64]               0\n",
      "          Conv2d-287         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-288         [-1, 2048, 24, 64]           4,096\n",
      "          Conv2d-289         [-1, 2048, 24, 64]       2,097,152\n",
      "     BatchNorm2d-290         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-291         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-292          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-293          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-294          [-1, 512, 24, 64]               0\n",
      "          Conv2d-295          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-296          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-297          [-1, 512, 24, 64]               0\n",
      "          Conv2d-298         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-299         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-300         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-301          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-302          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-303          [-1, 512, 24, 64]               0\n",
      "          Conv2d-304          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-305          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-306          [-1, 512, 24, 64]               0\n",
      "          Conv2d-307         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-308         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-309         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-310          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-311          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-312          [-1, 512, 24, 64]               0\n",
      "          Conv2d-313          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-314          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-315          [-1, 512, 24, 64]               0\n",
      "          Conv2d-316         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-317         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-318         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-319          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-320          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-321          [-1, 512, 24, 64]               0\n",
      "          Conv2d-322          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-323          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-324          [-1, 512, 24, 64]               0\n",
      "          Conv2d-325         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-326         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-327         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-328          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-329          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-330          [-1, 512, 24, 64]               0\n",
      "          Conv2d-331          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-332          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-333          [-1, 512, 24, 64]               0\n",
      "          Conv2d-334         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-335         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-336         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-337          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-338          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-339          [-1, 512, 24, 64]               0\n",
      "          Conv2d-340          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-341          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-342          [-1, 512, 24, 64]               0\n",
      "          Conv2d-343         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-344         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-345         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-346          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-347          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-348          [-1, 512, 24, 64]               0\n",
      "          Conv2d-349          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-350          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-351          [-1, 512, 24, 64]               0\n",
      "          Conv2d-352         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-353         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-354         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-355          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-356          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-357          [-1, 512, 24, 64]               0\n",
      "          Conv2d-358          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-359          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-360          [-1, 512, 24, 64]               0\n",
      "          Conv2d-361         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-362         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-363         [-1, 2048, 24, 64]               0\n",
      "================================================================\n",
      "Total params: 69,275,712\n",
      "Trainable params: 69,275,712\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.50\n",
      "Forward/backward pass size (MB): 3747.00\n",
      "Params size (MB): 264.27\n",
      "Estimated Total Size (MB): 4015.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNet_Atrous('resnet101', dilation_rate=[1,2,1])\n",
    "summary(model, (3,384,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df75ac43-1857-411b-aaf6-e14790a3a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预先训练模型地址\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "bn_mom = 0.0003\n",
    "model_paths = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14f88d-bfbd-4d7f-be9e-89d330de18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_pretrained(model_paths['resnet50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1659d0-ae05-47e3-8937-e89a1b1650b5",
   "metadata": {},
   "source": [
    "<img src='images/aspp.png' style='height:150px'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ddfdfa2c-251b-40a0-8e3b-37bd17e67ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atrous Spatial Pyramid Pooling 空洞空间金字塔池化\n",
    "class ASPP(nn.Module): \n",
    "    def __init__(self, in_chans, out_chans, rate=1):\n",
    "        super(ASPP, self).__init__()\n",
    "        kernel_size=[1,3,3,3]\n",
    "        dilation_rate = [i*rate for i in [1,6,12,18]]\n",
    "        padding = [0] + dilation_rate[1:] #首个valid之后，都是same conv\n",
    "        \n",
    "        # 对ResNet结果用不同d分别做空洞卷积，从而以多个比例感受野捕捉图像的上下文 (14,14,2048)->(14,14,256)\n",
    "        self.dilated_conv = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.dilated_conv.append(nn.Sequential(\n",
    "                 nn.Conv2d(in_chans, out_chans, kernel_size[i], 1, padding=padding[i], dilation=dilation_rate[i]), \n",
    "                 nn.BatchNorm2d(out_chans), \n",
    "                 nn.ReLU(inplace=True)))\n",
    "        \n",
    "        # 图像层级特征 (1,1,2048)->(1,1,256)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1) #默认keep_dims，h,w=1，相当于gap\n",
    "        self.branch5 = nn.Sequential(\n",
    "                 nn.Conv2d(in_chans, out_chans, 1, 1), \n",
    "                 nn.BatchNorm2d(out_chans), \n",
    "                 nn.ReLU(inplace=True))\n",
    "        \n",
    "        # 合并4个空洞卷积结果和1个gpa结果后，再做一次1x1卷积\n",
    "        self.branch6 = nn.Sequential(\n",
    "                 nn.Conv2d(out_chans*5, out_chans, 1, 1), \n",
    "                 nn.BatchNorm2d(out_chans), \n",
    "                 nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        b, c, h, w = inputs.shape \n",
    "        fusion = []\n",
    "        for layer in self.dilated_conv:\n",
    "            x = layer(inputs) #(14,14,256)\n",
    "            fusion.append(x)\n",
    "        \n",
    "        gap = self.avgpool(inputs)\n",
    "        gap = self.branch5(gap)\n",
    "        gap = nn.UpsamplingBilinear2d(size=[h,w])(gap) # 上采样到原始inputs大小\n",
    "        fusion.append(gap)\n",
    "        \n",
    "        x = torch.cat(fusion, dim=1) #(14,14,256*5)\n",
    "        x = self.branch6(x) #(14,14,256)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e07d9b8c-9d9b-4917-b909-a430cd4a0447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 24, 64]         524,544\n",
      "       BatchNorm2d-2          [-1, 256, 24, 64]             512\n",
      "              ReLU-3          [-1, 256, 24, 64]               0\n",
      "            Conv2d-4          [-1, 256, 24, 64]       4,718,848\n",
      "       BatchNorm2d-5          [-1, 256, 24, 64]             512\n",
      "              ReLU-6          [-1, 256, 24, 64]               0\n",
      "            Conv2d-7          [-1, 256, 24, 64]       4,718,848\n",
      "       BatchNorm2d-8          [-1, 256, 24, 64]             512\n",
      "              ReLU-9          [-1, 256, 24, 64]               0\n",
      "           Conv2d-10          [-1, 256, 24, 64]       4,718,848\n",
      "      BatchNorm2d-11          [-1, 256, 24, 64]             512\n",
      "             ReLU-12          [-1, 256, 24, 64]               0\n",
      "AdaptiveAvgPool2d-13           [-1, 2048, 1, 1]               0\n",
      "           Conv2d-14            [-1, 256, 1, 1]         524,544\n",
      "      BatchNorm2d-15            [-1, 256, 1, 1]             512\n",
      "             ReLU-16            [-1, 256, 1, 1]               0\n",
      "           Conv2d-17          [-1, 256, 24, 64]         327,936\n",
      "      BatchNorm2d-18          [-1, 256, 24, 64]             512\n",
      "             ReLU-19          [-1, 256, 24, 64]               0\n",
      "================================================================\n",
      "Total params: 15,536,640\n",
      "Trainable params: 15,536,640\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 12.00\n",
      "Forward/backward pass size (MB): 45.02\n",
      "Params size (MB): 59.27\n",
      "Estimated Total Size (MB): 116.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ASPP(2048,256)\n",
    "summary(model, (2048,24,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "80e53717-d137-4bf7-ad94-fe5626f131dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeplabV3Plus(nn.Module):\n",
    "    def __init__(self, backbone='resnet50', in_chans=3, num_classes=8, aspp_filters=256, \n",
    "                 shortcut_filters=48, shortcut_kernel_size=1, os=16):\n",
    "        super(DeeplabV3Plus, self).__init__()\n",
    "        \n",
    "        # ResNet提取特征\n",
    "        self.backbone = ResNet_Atrous(backbone, in_chans, dilation_rate=[1,2,1], os=os) #(224,224,3)->(14,14,2048)\n",
    "        \n",
    "        #L5 深层feature 从原图downsample 16倍 -> upsample 4倍\n",
    "        in_chans = 512*self.backbone.block.expansion #resnet的输出\n",
    "        self.aspp = ASPP(in_chans, aspp_filters, dilation_rate=16//os) #(14,14,2048)->(14,14,256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.upsample1 = nn.UpsamplingBilinear2d(scale_factor=os//4) #(56,56,256)\n",
    "        \n",
    "        #L2 浅层feature 从原图downsample 4倍 (56,56,64)->(56,56,48)\n",
    "        in_chans = 64*self.backbone.block.expansion #layer 2的输出\n",
    "        self.shortcut = nn.Sequential(\n",
    "                 nn.Conv2d(in_chans, shortcut_filters, shortcut_kernel_size, 1, \n",
    "                           padding=shortcut_kernel_size//2, bias=False), \n",
    "                 nn.BatchNorm2d(shortcut_filters), \n",
    "                 nn.ReLU(inplace=True)) \n",
    "        \n",
    "        # 合并feature (56,56,256+48)->(56,56,256) \n",
    "        self.concat = nn.Sequential(\n",
    "                 nn.Conv2d(aspp_filters+shortcut_filters, aspp_filters, 3, 1, padding=1, bias=False), \n",
    "                 nn.BatchNorm2d(aspp_filters), \n",
    "                 nn.ReLU(inplace=True),\n",
    "                 nn.Dropout(0.5),\n",
    "                 nn.Conv2d(aspp_filters, aspp_filters, 3, 1, padding=1, bias=False), \n",
    "                 nn.BatchNorm2d(aspp_filters), \n",
    "                 nn.ReLU(inplace=True),\n",
    "                 nn.Dropout(0.1))\n",
    "        \n",
    "        self.classifier = nn.Conv2d(aspp_filters, num_classes, 1, 1) #(56,56,n_classes)\n",
    "        self.upsample2 = nn.UpsamplingBilinear2d(scale_factor=4)  #(224,224,n_classes)\n",
    "\n",
    "        # 初始化参数\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "                \n",
    "    def forward(self, inputs):\n",
    "        # 深层features\n",
    "        f1, f2, f3, f4, f5 = self.backbone(inputs) \n",
    "        feature_aspp = self.aspp(f5)\n",
    "        feature_aspp = self.dropout(feature_aspp)\n",
    "        feature_aspp = self.upsample1(feature_aspp)\n",
    "\n",
    "        # 浅层features\n",
    "        feature_shallow = self.shortcut(f2)\n",
    "        \n",
    "        # 合并features\n",
    "        feature = torch.cat([feature_aspp, feature_shallow], axis=1)\n",
    "        x = self.concat(feature)\n",
    "        x = self.classifier(x)\n",
    "        x = self.upsample2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8a4afd76-61ef-45cf-80c3-54b1b21dc8c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 192, 512]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 192, 512]             128\n",
      "              ReLU-3         [-1, 64, 192, 512]               0\n",
      "         MaxPool2d-4          [-1, 64, 96, 256]               0\n",
      "            Conv2d-5          [-1, 64, 96, 256]           4,096\n",
      "       BatchNorm2d-6          [-1, 64, 96, 256]             128\n",
      "              ReLU-7          [-1, 64, 96, 256]               0\n",
      "            Conv2d-8          [-1, 64, 96, 256]          36,864\n",
      "       BatchNorm2d-9          [-1, 64, 96, 256]             128\n",
      "             ReLU-10          [-1, 64, 96, 256]               0\n",
      "           Conv2d-11         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-12         [-1, 256, 96, 256]             512\n",
      "           Conv2d-13         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-14         [-1, 256, 96, 256]             512\n",
      "       BottleNeck-15         [-1, 256, 96, 256]               0\n",
      "           Conv2d-16          [-1, 64, 96, 256]          16,384\n",
      "      BatchNorm2d-17          [-1, 64, 96, 256]             128\n",
      "             ReLU-18          [-1, 64, 96, 256]               0\n",
      "           Conv2d-19          [-1, 64, 96, 256]          36,864\n",
      "      BatchNorm2d-20          [-1, 64, 96, 256]             128\n",
      "             ReLU-21          [-1, 64, 96, 256]               0\n",
      "           Conv2d-22         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-23         [-1, 256, 96, 256]             512\n",
      "       BottleNeck-24         [-1, 256, 96, 256]               0\n",
      "           Conv2d-25          [-1, 64, 96, 256]          16,384\n",
      "      BatchNorm2d-26          [-1, 64, 96, 256]             128\n",
      "             ReLU-27          [-1, 64, 96, 256]               0\n",
      "           Conv2d-28          [-1, 64, 96, 256]          36,864\n",
      "      BatchNorm2d-29          [-1, 64, 96, 256]             128\n",
      "             ReLU-30          [-1, 64, 96, 256]               0\n",
      "           Conv2d-31         [-1, 256, 96, 256]          16,384\n",
      "      BatchNorm2d-32         [-1, 256, 96, 256]             512\n",
      "       BottleNeck-33         [-1, 256, 96, 256]               0\n",
      "           Conv2d-34         [-1, 128, 96, 256]          32,768\n",
      "      BatchNorm2d-35         [-1, 128, 96, 256]             256\n",
      "             ReLU-36         [-1, 128, 96, 256]               0\n",
      "           Conv2d-37         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-38         [-1, 128, 48, 128]             256\n",
      "             ReLU-39         [-1, 128, 48, 128]               0\n",
      "           Conv2d-40         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-41         [-1, 512, 48, 128]           1,024\n",
      "           Conv2d-42         [-1, 512, 48, 128]         131,072\n",
      "      BatchNorm2d-43         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-44         [-1, 512, 48, 128]               0\n",
      "           Conv2d-45         [-1, 128, 48, 128]          65,536\n",
      "      BatchNorm2d-46         [-1, 128, 48, 128]             256\n",
      "             ReLU-47         [-1, 128, 48, 128]               0\n",
      "           Conv2d-48         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-49         [-1, 128, 48, 128]             256\n",
      "             ReLU-50         [-1, 128, 48, 128]               0\n",
      "           Conv2d-51         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-52         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-53         [-1, 512, 48, 128]               0\n",
      "           Conv2d-54         [-1, 128, 48, 128]          65,536\n",
      "      BatchNorm2d-55         [-1, 128, 48, 128]             256\n",
      "             ReLU-56         [-1, 128, 48, 128]               0\n",
      "           Conv2d-57         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-58         [-1, 128, 48, 128]             256\n",
      "             ReLU-59         [-1, 128, 48, 128]               0\n",
      "           Conv2d-60         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-61         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-62         [-1, 512, 48, 128]               0\n",
      "           Conv2d-63         [-1, 128, 48, 128]          65,536\n",
      "      BatchNorm2d-64         [-1, 128, 48, 128]             256\n",
      "             ReLU-65         [-1, 128, 48, 128]               0\n",
      "           Conv2d-66         [-1, 128, 48, 128]         147,456\n",
      "      BatchNorm2d-67         [-1, 128, 48, 128]             256\n",
      "             ReLU-68         [-1, 128, 48, 128]               0\n",
      "           Conv2d-69         [-1, 512, 48, 128]          65,536\n",
      "      BatchNorm2d-70         [-1, 512, 48, 128]           1,024\n",
      "       BottleNeck-71         [-1, 512, 48, 128]               0\n",
      "           Conv2d-72         [-1, 256, 48, 128]         131,072\n",
      "      BatchNorm2d-73         [-1, 256, 48, 128]             512\n",
      "             ReLU-74         [-1, 256, 48, 128]               0\n",
      "           Conv2d-75          [-1, 256, 24, 64]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 24, 64]             512\n",
      "             ReLU-77          [-1, 256, 24, 64]               0\n",
      "           Conv2d-78         [-1, 1024, 24, 64]         262,144\n",
      "      BatchNorm2d-79         [-1, 1024, 24, 64]           2,048\n",
      "           Conv2d-80         [-1, 1024, 24, 64]         524,288\n",
      "      BatchNorm2d-81         [-1, 1024, 24, 64]           2,048\n",
      "       BottleNeck-82         [-1, 1024, 24, 64]               0\n",
      "           Conv2d-83          [-1, 256, 24, 64]         262,144\n",
      "      BatchNorm2d-84          [-1, 256, 24, 64]             512\n",
      "             ReLU-85          [-1, 256, 24, 64]               0\n",
      "           Conv2d-86          [-1, 256, 24, 64]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 24, 64]             512\n",
      "             ReLU-88          [-1, 256, 24, 64]               0\n",
      "           Conv2d-89         [-1, 1024, 24, 64]         262,144\n",
      "      BatchNorm2d-90         [-1, 1024, 24, 64]           2,048\n",
      "       BottleNeck-91         [-1, 1024, 24, 64]               0\n",
      "           Conv2d-92          [-1, 256, 24, 64]         262,144\n",
      "      BatchNorm2d-93          [-1, 256, 24, 64]             512\n",
      "             ReLU-94          [-1, 256, 24, 64]               0\n",
      "           Conv2d-95          [-1, 256, 24, 64]         589,824\n",
      "      BatchNorm2d-96          [-1, 256, 24, 64]             512\n",
      "             ReLU-97          [-1, 256, 24, 64]               0\n",
      "           Conv2d-98         [-1, 1024, 24, 64]         262,144\n",
      "      BatchNorm2d-99         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-100         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-101          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 24, 64]             512\n",
      "            ReLU-103          [-1, 256, 24, 64]               0\n",
      "          Conv2d-104          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 24, 64]             512\n",
      "            ReLU-106          [-1, 256, 24, 64]               0\n",
      "          Conv2d-107         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-109         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-110          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-111          [-1, 256, 24, 64]             512\n",
      "            ReLU-112          [-1, 256, 24, 64]               0\n",
      "          Conv2d-113          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-114          [-1, 256, 24, 64]             512\n",
      "            ReLU-115          [-1, 256, 24, 64]               0\n",
      "          Conv2d-116         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-117         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-118         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-119          [-1, 256, 24, 64]         262,144\n",
      "     BatchNorm2d-120          [-1, 256, 24, 64]             512\n",
      "            ReLU-121          [-1, 256, 24, 64]               0\n",
      "          Conv2d-122          [-1, 256, 24, 64]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 24, 64]             512\n",
      "            ReLU-124          [-1, 256, 24, 64]               0\n",
      "          Conv2d-125         [-1, 1024, 24, 64]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 24, 64]           2,048\n",
      "      BottleNeck-127         [-1, 1024, 24, 64]               0\n",
      "          Conv2d-128          [-1, 512, 24, 64]         524,288\n",
      "     BatchNorm2d-129          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-130          [-1, 512, 24, 64]               0\n",
      "          Conv2d-131          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-132          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-133          [-1, 512, 24, 64]               0\n",
      "          Conv2d-134         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-135         [-1, 2048, 24, 64]           4,096\n",
      "          Conv2d-136         [-1, 2048, 24, 64]       2,097,152\n",
      "     BatchNorm2d-137         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-138         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-139          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-140          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-141          [-1, 512, 24, 64]               0\n",
      "          Conv2d-142          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-143          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-144          [-1, 512, 24, 64]               0\n",
      "          Conv2d-145         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-146         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-147         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-148          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-149          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-150          [-1, 512, 24, 64]               0\n",
      "          Conv2d-151          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-152          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-153          [-1, 512, 24, 64]               0\n",
      "          Conv2d-154         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-155         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-156         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-157          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-158          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-159          [-1, 512, 24, 64]               0\n",
      "          Conv2d-160          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-161          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-162          [-1, 512, 24, 64]               0\n",
      "          Conv2d-163         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-164         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-165         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-166          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-167          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-168          [-1, 512, 24, 64]               0\n",
      "          Conv2d-169          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-170          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-171          [-1, 512, 24, 64]               0\n",
      "          Conv2d-172         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-173         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-174         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-175          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-176          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-177          [-1, 512, 24, 64]               0\n",
      "          Conv2d-178          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-179          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-180          [-1, 512, 24, 64]               0\n",
      "          Conv2d-181         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-182         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-183         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-184          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-185          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-186          [-1, 512, 24, 64]               0\n",
      "          Conv2d-187          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-188          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-189          [-1, 512, 24, 64]               0\n",
      "          Conv2d-190         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-191         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-192         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-193          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-194          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-195          [-1, 512, 24, 64]               0\n",
      "          Conv2d-196          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-197          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-198          [-1, 512, 24, 64]               0\n",
      "          Conv2d-199         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-200         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-201         [-1, 2048, 24, 64]               0\n",
      "          Conv2d-202          [-1, 512, 24, 64]       1,048,576\n",
      "     BatchNorm2d-203          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-204          [-1, 512, 24, 64]               0\n",
      "          Conv2d-205          [-1, 512, 24, 64]       2,359,296\n",
      "     BatchNorm2d-206          [-1, 512, 24, 64]           1,024\n",
      "            ReLU-207          [-1, 512, 24, 64]               0\n",
      "          Conv2d-208         [-1, 2048, 24, 64]       1,048,576\n",
      "     BatchNorm2d-209         [-1, 2048, 24, 64]           4,096\n",
      "      BottleNeck-210         [-1, 2048, 24, 64]               0\n",
      "   ResNet_Atrous-211  [[-1, 64, 192, 512], [-1, 256, 96, 256], [-1, 512, 48, 128], [-1, 1024, 24, 64], [-1, 2048, 24, 64]]               0\n",
      "          Conv2d-212          [-1, 256, 24, 64]         524,544\n",
      "     BatchNorm2d-213          [-1, 256, 24, 64]             512\n",
      "            ReLU-214          [-1, 256, 24, 64]               0\n",
      "          Conv2d-215          [-1, 256, 24, 64]       4,718,848\n",
      "     BatchNorm2d-216          [-1, 256, 24, 64]             512\n",
      "            ReLU-217          [-1, 256, 24, 64]               0\n",
      "          Conv2d-218          [-1, 256, 24, 64]       4,718,848\n",
      "     BatchNorm2d-219          [-1, 256, 24, 64]             512\n",
      "            ReLU-220          [-1, 256, 24, 64]               0\n",
      "          Conv2d-221          [-1, 256, 24, 64]       4,718,848\n",
      "     BatchNorm2d-222          [-1, 256, 24, 64]             512\n",
      "            ReLU-223          [-1, 256, 24, 64]               0\n",
      "AdaptiveAvgPool2d-224           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-225            [-1, 256, 1, 1]         524,544\n",
      "     BatchNorm2d-226            [-1, 256, 1, 1]             512\n",
      "            ReLU-227            [-1, 256, 1, 1]               0\n",
      "          Conv2d-228          [-1, 256, 24, 64]         327,936\n",
      "     BatchNorm2d-229          [-1, 256, 24, 64]             512\n",
      "            ReLU-230          [-1, 256, 24, 64]               0\n",
      "            ASPP-231          [-1, 256, 24, 64]               0\n",
      "         Dropout-232          [-1, 256, 24, 64]               0\n",
      "UpsamplingBilinear2d-233         [-1, 256, 96, 256]               0\n",
      "          Conv2d-234          [-1, 48, 96, 256]          12,288\n",
      "     BatchNorm2d-235          [-1, 48, 96, 256]              96\n",
      "            ReLU-236          [-1, 48, 96, 256]               0\n",
      "          Conv2d-237         [-1, 256, 96, 256]         700,416\n",
      "     BatchNorm2d-238         [-1, 256, 96, 256]             512\n",
      "            ReLU-239         [-1, 256, 96, 256]               0\n",
      "         Dropout-240         [-1, 256, 96, 256]               0\n",
      "          Conv2d-241         [-1, 256, 96, 256]         589,824\n",
      "     BatchNorm2d-242         [-1, 256, 96, 256]             512\n",
      "            ReLU-243         [-1, 256, 96, 256]               0\n",
      "         Dropout-244         [-1, 256, 96, 256]               0\n",
      "          Conv2d-245           [-1, 8, 96, 256]           2,056\n",
      "UpsamplingBilinear2d-246         [-1, 8, 384, 1024]               0\n",
      "================================================================\n",
      "Total params: 67,125,928\n",
      "Trainable params: 67,125,928\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.50\n",
      "Forward/backward pass size (MB): 3364.52\n",
      "Params size (MB): 256.07\n",
      "Estimated Total Size (MB): 3625.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = DeeplabV3Plus('resnet50')\n",
    "summary(model, (3,384,1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0dfd1-0041-46e0-b539-e29eb53f293b",
   "metadata": {},
   "source": [
    "#### TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44256d9-9f4c-41c4-8e1a-324803f94aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 适用于ResNet18,34\n",
    "class BasicBlock(keras.Model):\n",
    "    expansion=1\n",
    "    def __init__(self, filters, strides, kernel_size=3, downsampling=False, dilation_rate=1): \n",
    "        super(BasicBlock, self).__init__()\n",
    "        #第一层stride有时候需要downsample，而第二层stride默认1\n",
    "        #tf的padding神奇在于，选择same和stride=2时，相当于减半而非不变\n",
    "        self.conv1 = layers.Conv2D(filters, kernel_size, strides, padding='same', use_bias=False,\n",
    "                                  dilation_rate=dilation_rate) #可以增加dilation_rate做空洞卷积\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(filters*BasicBlock.expansion, kernel_size, strides=1, padding='same',use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu') #可以重复使用，因为只是个函数，没有weights\n",
    "        self.add = layers.Add()\n",
    "        \n",
    "        #当downsampling时，需要用ConvBlock\n",
    "        if downsampling:\n",
    "            self.shortcut = keras.Sequential([\n",
    "                layers.Conv2D(filters, 1, strides, use_bias=False),\n",
    "                layers.BatchNormalization()])\n",
    "        else:\n",
    "            self.shortcut = lambda x:x\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.add([x, self.shortcut(inputs)])\n",
    "        x = self.relu(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bc95f3-bde3-4289-945d-8a721fa62c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 适用于ResNet50,101,152\n",
    "class BottleNeck(keras.Model):\n",
    "    expansion=4\n",
    "    def __init__(self, filters, strides, kernel_size=3, downsampling=False, dilation_rate=1): \n",
    "        super(BottleNeck, self).__init__()\n",
    "        #瓶颈 feature map\n",
    "        self.conv1 = layers.Conv2D(filters, 1, strides=1, padding='same', use_bias=False) #降c\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(filters, kernel_size, strides, padding='same', use_bias=False, #降hw\n",
    "                                   dilation_rate=dilation_rate) #可以增加空洞卷积，对应strides=1\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(filters*BottleNeck.expansion, 1, strides=1, padding='same', use_bias=False) #增c\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        self.add = layers.Add()\n",
    "       \n",
    "        #当downsampling时，需要用ConvBlock\n",
    "        if downsampling:\n",
    "            self.shortcut = keras.Sequential([\n",
    "                layers.Conv2D(filters*BottleNeck.expansion, 1, strides, use_bias=False),\n",
    "                layers.BatchNormalization(axis=3)])\n",
    "        else:\n",
    "            self.shortcut = lambda x:x\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.add([x, self.shortcut(inputs)])\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0f45b6-1a03-41cf-bafe-fcbb1629bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(keras.Model):    \n",
    "    def __init__(self, name, block=None, n_blocks=None,  \n",
    "                 dilation_rate=[1,1,1], #若增加空洞卷积，layer 5直接指定每个block的d\n",
    "                 os=16): #output_stride 可选下采样倍数os=8/16，传统resnet都是2**5=32倍downsampling\n",
    "        super(ResNet, self).__init__(name=name) \n",
    "        \n",
    "        # 结构：使用自定义的结构，或者经典结构\n",
    "        self.structure = name\n",
    "        self.block = block #用basicblock还是bottleneck\n",
    "        self.n_blocks = n_blocks #每个block循环次数\n",
    "        self.get_structure() #重新整理结构\n",
    "        strides = self.get_strides(os, dilation_rate)\n",
    "        \n",
    "        # Stem\n",
    "        self.layer1 = keras.Sequential(\n",
    "            [layers.ZeroPadding2D(3), #由于Conv2D内无法自定义padding=3，需要先另外做padding \n",
    "             layers.Conv2D(64, 7, 2, use_bias=False), #2倍downsample\n",
    "             layers.BatchNormalization(), \n",
    "             layers.Activation('relu'), \n",
    "             layers.MaxPool2D(3, 2, padding='same')],name='layer1') #4倍downsample\n",
    "            \n",
    "        #除了layer2因为有事先pooling降维之外，其余layer都是第一个block的stride=2降维，其余blocks的stride=1\n",
    "        self.layer2 = self.build_blocks(2, 64, self.n_blocks[0], strides=strides[0])         \n",
    "        self.layer3 = self.build_blocks(3, 128, self.n_blocks[1], strides=strides[1]) #8倍downsample，根据下面layers决定最终downsample倍数   \n",
    "        self.layer4 = self.build_blocks(4, 256, self.n_blocks[2], strides=strides[2], \n",
    "                                        dilation_rate=16//os) #可以对每个block做空洞d，因为不同resnet结构layer4的n_blocks不同，在函数另外构造d列表     \n",
    "        self.layer5 = self.build_blocks(5, 512, self.n_blocks[3], strides=strides[3], \n",
    "                                        #dilation_rate=dilation_rate) #由于layer5统一都是3个blocks，直接指定每个block的空洞d，如[1,2,1]\n",
    "                                        dilation_rate=[i*16//os for i in dilation_rate]) #若下采样os=8倍为止，那么这层的dilated_rate要乘以2\n",
    "        #若用空洞卷积，还可以自定义额外增加几次最后一个layer结构\n",
    "        self.layer6 = self.build_blocks(6, 512, self.n_blocks[3], strides=strides[3], dilation_rate=[i*16//os for i in dilation_rate]) #同layer5\n",
    "        self.layer7 = self.build_blocks(7, 512, self.n_blocks[3], strides=strides[3], dilation_rate=[i*16//os for i in dilation_rate]) \n",
    "        \n",
    "    \n",
    "    def build_blocks(self, idx, filters, n_blocks, strides, dilation_rate=1):\n",
    "        if isinstance(dilation_rate, int): #根据具体blocks数构造\n",
    "            dilation_rate=[dilation_rate]*n_blocks\n",
    "            \n",
    "        res_blocks = keras.Sequential(name=f'layer{idx}') \n",
    "        res_blocks.add(self.block(filters, strides, downsampling=True, dilation_rate=dilation_rate[0])) \n",
    "        for i in range(1, n_blocks): #跳过第一个block            \n",
    "            res_blocks.add(self.block(filters, strides=1, dilation_rate=dilation_rate[i])) \n",
    "        return res_blocks\n",
    "    \n",
    "    \n",
    "    def get_structure(self):\n",
    "        versions = {'resnet18':(BasicBlock,[2,2,2,2]), \n",
    "                     'resnet34':(BasicBlock,[3,4,6,3]),\n",
    "                     'resnet50':(BottleNeck,[3,4,6,3]),\n",
    "                     'resnet101':(BottleNeck,[3,4,23,3]),\n",
    "                     'resnet152':(BottleNeck,[3,8,36,3])}\n",
    "        \n",
    "        if not self.block or not self.n_blocks: #如果任意一项没有定义\n",
    "            self.block, self.n_blocks = versions[self.structure]\n",
    "        \n",
    "        \n",
    "    def get_strides(self, os, dilation_rate):\n",
    "        if os==16 and dilation_rate==[1,1,1]: #代表用传统的32倍downsample，不用空洞卷积\n",
    "            strides=[1,2,2,2]\n",
    "        elif os==16: #若16倍downsample，仅最后layer 5 strides=1，可以用空洞卷积\n",
    "            strides=[1,2,2,1] \n",
    "        elif os==8: #若8倍downsample，最后layer 4-5 strides=1，可以用空洞卷积\n",
    "            strides=[1,2,1,1] \n",
    "        return strides\n",
    "   \n",
    "\n",
    "    def call(self, inputs): \n",
    "        x = self.layer1(inputs) #4倍  \n",
    "        l2 = self.layer2(x) #4倍         \n",
    "        l3 = self.layer3(l2) #8倍        \n",
    "        l4 = self.layer4(l3) #8/16倍        \n",
    "        x = self.layer5(l4) #8/16/32倍\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        return [l2, l3, l4, x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0eff2ab-afe5-41b6-baa9-cfbfa9661ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atrous Spatial Pyramid Pooling 空洞空间金字塔池化\n",
    "class ASPP(keras.Model): \n",
    "    def __init__(self, filters, dilation_rate=1):\n",
    "        super(ASPP, self).__init__(name='ASPP')\n",
    "        kernel_size=[1,3,3,3]\n",
    "        padding = ['valid','same','same','same']\n",
    "        dilation_rate = [1*dilation_rate,6*dilation_rate,12*dilation_rate,18*dilation_rate]\n",
    "        \n",
    "        # 对ResNet结果用不同d分别做空洞卷积，从而以多个比例感受野捕捉图像的上下文 (14,14,2048)->(14,14,256)\n",
    "        self.branch1 = keras.Sequential([\n",
    "                 layers.Conv2D(filters, kernel_size[0], 1, padding=padding[0], dilation_rate=dilation_rate[0],use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu')],name='branch1') \n",
    "        self.branch2 = keras.Sequential([\n",
    "                 layers.Conv2D(filters, kernel_size[1], 1, padding=padding[1], dilation_rate=dilation_rate[1], use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu')],name='branch2') \n",
    "        self.branch3 = keras.Sequential([\n",
    "                 layers.Conv2D(filters, kernel_size[2], 1, padding=padding[2], dilation_rate=dilation_rate[2], use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu')],name='branch3') \n",
    "        self.branch4 = keras.Sequential([\n",
    "                 layers.Conv2D(filters, kernel_size[3], 1, padding=padding[3], dilation_rate=dilation_rate[3], use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu')],name='branch4') \n",
    "        \n",
    "        # 图像层级特征 (1,1,2048)->(1,1,256)\n",
    "        self.avgpool = layers.GlobalAveragePooling2D(keepdims=True) \n",
    "        self.branch5 = keras.Sequential(\n",
    "                [layers.Conv2D(filters, 1, 1, use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu')],name='branch5') \n",
    "        \n",
    "        # 合并4个空洞卷积结果和1个gpa结果后，再做一次1x1卷积\n",
    "        self.branch6 = keras.Sequential(\n",
    "                [layers.Conv2D(filters, 1, 1, use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu')],name='branch6') \n",
    "\n",
    "    def call(self, inputs):\n",
    "        b, h, w, c = inputs.shape \n",
    "        c1 = self.branch1(inputs) #(14,14,256)\n",
    "        c2 = self.branch2(inputs)\n",
    "        c3 = self.branch3(inputs)\n",
    "        c4 = self.branch4(inputs)\n",
    "        gap = self.avgpool(inputs)\n",
    "        gap = self.branch5(gap)\n",
    "        gap = layers.UpSampling2D(size=(h,w),interpolation='bilinear')(gap) # 上采样到原始inputs大小\n",
    "        \n",
    "        x = layers.concatenate([c1,c2,c3,c4,gap]) #(14,14,256*5)\n",
    "        x = self.branch6(x) #(14,14,256)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4d8235-7e60-4280-a821-a7b9f9b851fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeplabV3(keras.Model):\n",
    "    def __init__(self, backbone=config.BACKBONE, num_classes=config.NUM_CLASSES, aspp_filters=config.ASPP_OUTDIM, \n",
    "                 shortcut_filters=config.SHORTCUT_DIM, shortcut_kernel=config.SHORTCUT_KERNEL, os=config.OUTPUT_STRIDE):\n",
    "        super(DeeplabV3, self).__init__(name='DeepLabV3')\n",
    "        \n",
    "        #L5 深层feature 从原图downsample 16倍 -> upsample 4倍\n",
    "        self.backbone = ResNet(backbone, dilation_rate=[1,2,1], os=os) #(224,224,3)->(14,14,2048)\n",
    "        self.aspp = ASPP(aspp_filters, dilation_rate=16//os) #(14,14,2048)->(14,14,256)\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.upsample1 = layers.UpSampling2D(size=(os//4,os//4),interpolation='bilinear') #(56,56,256)\n",
    "        \n",
    "        #L1 浅层feature 从原图downsample 4倍 (56,56,64)->(56,56,48)\n",
    "        self.shortcut = keras.Sequential([ \n",
    "                 layers.Conv2D(shortcut_filters, shortcut_kernel, 1, padding='same', use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu')],name='shortcut') \n",
    "        \n",
    "        # 合并feature (56,56,256+48)->(56,56,256) \n",
    "        self.concat = keras.Sequential([ \n",
    "                 layers.Conv2D(aspp_filters, 3, 1, padding='same', use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu'),\n",
    "                 layers.Dropout(0.5),\n",
    "                 layers.Conv2D(aspp_filters, 3, 1, padding='same', use_bias=False), \n",
    "                 layers.BatchNormalization(), \n",
    "                 layers.Activation('relu'),\n",
    "                 layers.Dropout(0.1)],name='concat')\n",
    "        self.classifier = layers.Conv2D(num_classes, 1, 1, padding='same', use_bias=False,name='classifier') #(56,56,n_classes)\n",
    "        self.upsample2 = layers.UpSampling2D(size=(4,4),interpolation='bilinear') #(224,224,n_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 深层features\n",
    "        l2,l3,l4,l7 = self.backbone(inputs) \n",
    "        feature_aspp = self.aspp(l7)\n",
    "        feature_aspp = self.dropout(feature_aspp)\n",
    "        feature_aspp = self.upsample1(feature_aspp)\n",
    "\n",
    "        # 浅层features\n",
    "        feature_shallow = self.shortcut(l2)\n",
    "        \n",
    "        # 合并features\n",
    "        feature = layers.concatenate([feature_aspp, feature_shallow], axis=-1)\n",
    "        x = self.concat(feature)\n",
    "        x = self.classifier(x)\n",
    "        x = self.upsample2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a571482d-ac5c-4766-a8ab-c0f7c057038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 15:21:35.427908: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 384, 1024, 8) dtype=float32 (created by layer 'up_sampling2d_1')>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepLabV3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (ResNet)            [(None, 96, 256, 256), (N 58778560  \n",
      "_________________________________________________________________\n",
      "ASPP (ASPP)                  (None, 24, 64, 256)       15538176  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 96, 256, 256)      0         \n",
      "_________________________________________________________________\n",
      "shortcut (Sequential)        (None, 96, 256, 48)       12480     \n",
      "_________________________________________________________________\n",
      "concat (Sequential)          (None, 96, 256, 256)      1292288   \n",
      "_________________________________________________________________\n",
      "classifier (Conv2D)          (None, 96, 256, 8)        2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 384, 1024, 8)      0         \n",
      "=================================================================\n",
      "Total params: 75,623,552\n",
      "Trainable params: 75,521,184\n",
      "Non-trainable params: 102,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(384,1024,3))\n",
    "model = DeeplabV3(backbone='resnet50', os=16)\n",
    "model.build(input_shape=(None,384,1024,3))\n",
    "model.call(inputs)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
